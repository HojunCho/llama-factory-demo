{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3922ba",
   "metadata": {},
   "source": [
    "# 이미지 분류 멀티모달 LLM 파인튜닝 튜토리얼\n",
    "\n",
    "이 튜토리얼에서는 **vLLM**과 **LLaMA-Factory**를 활용하여 멀티모달 LLM(MLLM)을 이미지 분류 작업에 특화되도록 파인튜닝하는 방법을 학습합니다.\n",
    "\n",
    "## 1. vLLM 서버 실행 및 연결 테스트\n",
    "\n",
    "먼저 **Qwen2.5-VL-7B-Instruct** 모델을 사용하여 vLLM 서버를 실행합니다. 이 모델은 텍스트와 이미지를 모두 처리할 수 있는 멀티모달 모델입니다.\n",
    "\n",
    "### 사전 준비사항\n",
    "1. 🤗 Hugging Face에 가입하여 액세스 토큰을 발급받으세요\n",
    "2. [Qwen2.5-VL-7B-Instruct 페이지](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)에서 모델을 확인하세요\n",
    "\n",
    "### vLLM 서버 실행 명령어 설명\n",
    "- `HF_TOKEN`: 발급받은 Hugging Face 토큰을 입력하세요\n",
    "- `CUDA_VISIBLE_DEVICES`: 사용할 GPU 번호 (전체 GPU 사용시 생략 가능)  \n",
    "- `-tp`: 텐서 병렬화에 사용할 GPU 개수를 지정합니다\n",
    "\n",
    "**⚠️ 중요**: 아래 명령어를 별도의 터미널에서 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2bca8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_TOKEN=<허깅페이스 토큰>\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve Qwen/Qwen2.5-VL-7B-Instruct -tp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6149476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain-openai 라이브러리 임포트\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM 서버 주소\n",
    "    api_key=\"EMPTY\",  # vLLM에서는 임의의 값 사용\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",  # 모델 이름\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 이미지 파일을 base64로 인코딩\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "## data/image_classifiion/input.jpg를 활용해보자.\n",
    "image = Image.open(\"data/image_classification/input.jpg\")\n",
    "buffer = BytesIO()\n",
    "image.save(buffer, format=\"JPEG\")\n",
    "base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# 채팅 메시지 보내기\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"다음 이미지를 설명해줘:\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "        }\n",
    "    ]\n",
    ")]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"AI 응답:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083cac3",
   "metadata": {},
   "source": [
    "### ✅ vLLM 서버 실행 확인\n",
    "\n",
    "터미널에서 다음과 같은 로그가 출력되면 vLLM 서버가 정상적으로 시작된 것입니다:\n",
    "\n",
    "```log\n",
    "INFO:     Started server process [2609659]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```\n",
    "\n",
    "이제 **Langchain**을 사용하여 실행 중인 vLLM 서버에 이미지를 포함한 멀티모달 요청을 보내 정상 작동을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bd97c",
   "metadata": {},
   "source": [
    "### 📊 서버 요청 처리 확인\n",
    "\n",
    "위 코드를 실행한 후, vLLM을 실행시킨 터미널에서 다음과 같은 로그가 출력되면 멀티모달 요청이 성공적으로 처리된 것입니다:\n",
    "\n",
    "```log\n",
    "INFO 08-13 23:26:51 [logger.py:41] Received request chatcmpl-543715af49a8443da65b81033b0cd2de: \n",
    "prompt: '<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>다음 이미지를 설명해줘:<|im_end|>...\n",
    "INFO 08-13 23:26:51 [async_llm.py:269] Added request chatcmpl-543715af49a8443da65b81033b0cd2de.\n",
    "INFO:     127.0.0.1:39490 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
    "```\n",
    "\n",
    "## 2. 실습 데이터셋 및 목표 설정\n",
    "\n",
    "### 📋 사용 데이터셋\n",
    "이번 실습에서는 [sfarrukhm/intel-image-classification](https://huggingface.co/datasets/sfarrukhm/intel-image-classification) 데이터셋을 활용합니다.\n",
    "\n",
    "### 🎯 실습 목표\n",
    "자연 이미지를 6개 카테고리로 분류하는 멀티모달 모델을 구축하는 것이 목표입니다.\n",
    "\n",
    "**분류 카테고리:**\n",
    "- **Mountain** (산)\n",
    "- **Glacier** (빙하) \n",
    "- **Street** (거리)\n",
    "- **Sea** (바다)\n",
    "- **Forest** (숲)\n",
    "- **Buildings** (건물)\n",
    "\n",
    "먼저 샘플 이미지를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb907c",
   "metadata": {},
   "source": [
    "## 3. Zero-shot 정확도 평가\n",
    "\n",
    "### 🧪 실험 1: 기본 Zero-shot 예측\n",
    "\n",
    "먼저 **fine-tuning 없이** 기본 Qwen2.5-VL 모델이 이미지 분류를 얼마나 잘 수행할 수 있는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cc98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "system_prompt = \"주어진 이미지에 대해서 다음 중 하나로 분류하시오: ['Mountain', 'Glacier', 'Street', 'Sea', 'Forest', 'Buildings']\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(\n",
    "        content=[{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "        }]\n",
    "    )\n",
    "]\n",
    "\n",
    "# LLM에 요청\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c42c75",
   "metadata": {},
   "source": [
    "### 🔧 실험 2: Constrained Decoding으로 Zero-shot 예측\n",
    "\n",
    "기본 zero-shot 출력에서는 다음과 같은 문제가 발생할 수 있습니다:\n",
    "- 정해진 6개 카테고리 외의 답변 생성\n",
    "- 불필요한 설명이 포함된 긴 응답\n",
    "- 일관성 없는 출력 형태\n",
    "\n",
    "이러한 문제를 해결하기 위해 **Constrained Decoding**을 사용하여 출력을 6개 카테고리 중 하나로 강제로 제한해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1695158",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['Mountain', 'Glacier', 'Street', 'Sea', 'Forest', 'Buildings']\n",
    "\n",
    "constrained_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM 서버 주소\n",
    "    api_key=\"EMPTY\",  # vLLM에서는 임의의 값 사용\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",  # 모델 이름\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_choice\": options\n",
    "    }\n",
    ")\n",
    "\n",
    "# LLM에 요청\n",
    "response = constrained_llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756c4f2",
   "metadata": {},
   "source": [
    "이제 **Fine-tuning**을 통해 이미지 분류 성능을 개선해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913de17f",
   "metadata": {},
   "source": [
    "## 4. LLaMA-Factory를 활용한 Fine-tuning\n",
    "\n",
    "### 🛠️ 학습 데이터 준비\n",
    "\n",
    "이제 **LLaMA-Factory**를 사용하여 이미지 분류 성능을 향상시키기 위한 fine-tuning을 진행합니다.\n",
    "\n",
    "먼저 다음 명령어를 실행하여 멀티모달 학습용 데이터셋을 생성하겠습니다.\n",
    "\n",
    "**참고 자료**: [LLaMA-Factory 멀티모달 데이터셋 설정 예시](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/mllm_demo.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a4be1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 생성 코드\n",
    "python -m demo.create_image_classificat_dataset\n",
    "# 생성된 데이터 확인\n",
    "head -20 data/image_classification/train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1085d",
   "metadata": {},
   "source": [
    "### ⚙️ Fine-tuning 실행\n",
    "\n",
    "생성된 데이터셋을 확인했으니, 이제 실제 fine-tuning을 시작합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674d5d6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# demo/image_classification.yaml 파일을 확인해보자.\n",
    "CUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train demo/image_classification.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf73cc",
   "metadata": {},
   "source": [
    "### 🚀 Fine-tuned 모델(LoRA) 서빙\n",
    "\n",
    "학습이 완료되면 vLLM에 **LoRA 어댑터**를 추가하여 fine-tuned 모델을 서빙합니다.\n",
    "\n",
    "다음 명령어로 LoRA가 적용된 vLLM 서버를 실행하세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7e3ae",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_TOKEN=<허깅페이스 토큰>\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve Qwen/Qwen2.5-VL-7B-Instruct -tp 2 --enable-lora --lora-modules image_classification=$PWD/save/qwen-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692159e",
   "metadata": {},
   "source": [
    "## 5. 성능 비교 및 평가\n",
    "\n",
    "### 🔍 Base Model vs Fine-tuned Model 비교\n",
    "\n",
    "vLLM 서버가 LoRA와 함께 실행되면, 다음 두 모델의 성능을 직접 비교해보겠습니다:\n",
    "\n",
    "1. **Base Model**: 원본 Qwen2.5-VL-7B-Instruct\n",
    "2. **Fine-tuned Model**: LoRA가 적용된 모델\n",
    "\n",
    "동일한 입력에 대한 두 모델의 출력을 비교하여 fine-tuning의 효과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['Mountain', 'Glacier', 'Street', 'Sea', 'Forest', 'Buildings']\n",
    "\n",
    "constrained_lora_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM 서버 주소\n",
    "    api_key=\"EMPTY\",  # vLLM에서는 임의의 값 사용\n",
    "    model=\"image_classification\",  # Lora 이름\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_choice\": options\n",
    "    }\n",
    ")\n",
    "\n",
    "# LLM에 요청\n",
    "response = constrained_lora_llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641aeb0",
   "metadata": {},
   "source": [
    "### 📏 정량적 성능 평가\n",
    "\n",
    "학습이 제대로 되었는지 확인하기 위해 `data/image_classification/test.json`을 활용하여 두 모델의 정확도를 비교해보겠습니다.\n",
    "\n",
    "**평가 방법:**\n",
    "- 테스트 데이터셋의 각 이미지에 대해 예측 수행\n",
    "- Base model과 Fine-tuned model의 정확도 측정\n",
    "- 배치 처리를 통한 효율적인 평가 진행\n",
    "\n",
    "이를 통해 fine-tuning이 실제로 성능 향상에 기여했는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3302f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# test.json 파일 로드\n",
    "with open('data/image_classification/test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"총 테스트 샘플 수: {len(test_data)}\")\n",
    "\n",
    "# 이미지를 base64로 인코딩하는 함수\n",
    "def encode_image_to_base64(image_path):\n",
    "    full_path = os.path.join('data', image_path)\n",
    "    image = Image.open(full_path)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return base64_image\n",
    "\n",
    "# 배치 예측 함수\n",
    "def predict_batch_with_model(llm_model, image_paths, system_prompt):\n",
    "    \"\"\"배치로 이미지들을 한 번에 처리\"\"\"\n",
    "    batch_messages = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(\n",
    "                content=[{\n",
    "                    \"type\": \"image_url\", \n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "                }]\n",
    "            )\n",
    "        ]\n",
    "        batch_messages.append(messages)\n",
    "    \n",
    "    # 배치로 한 번에 처리\n",
    "    responses = llm_model.batch(batch_messages)\n",
    "    return [response.content.strip() for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델 (베이스라인) 평가\n",
    "print(\"=== 원본 모델 (Qwen2.5-VL-7B-Instruct) 평가 ===\")\n",
    "\n",
    "options = ['Mountain', 'Glacier', 'Street', 'Sea', 'Forest', 'Buildings']\n",
    "system_prompt = \"주어진 이미지에 대해서 다음 중 하나로 분류하시오: ['Mountain', 'Glacier', 'Street', 'Sea', 'Forest', 'Buildings']\"\n",
    "\n",
    "# 원본 모델 설정\n",
    "constrained_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\", \n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_choice\": options\n",
    "    }\n",
    ")\n",
    "\n",
    "correct_baseline = 0\n",
    "total_samples = len(test_data)\n",
    "\n",
    "# 테스트할 샘플 수 제한 (전체 데이터가 너무 클 경우)\n",
    "batch_size = 30  # 배치 크기\n",
    "\n",
    "print(f\"평가할 샘플 수: {total_samples}\")\n",
    "print(f\"배치 크기: {batch_size}\")\n",
    "\n",
    "# 배치 단위로 처리\n",
    "for batch_start in tqdm(range(0, total_samples, batch_size), desc=\"원본 모델 평가 (배치)\"):\n",
    "    batch_end = min(batch_start + batch_size, total_samples)\n",
    "    batch_samples = test_data[batch_start:batch_end]\n",
    "    \n",
    "    # 배치 데이터 준비\n",
    "    batch_image_paths = [sample['images'][0] for sample in batch_samples]\n",
    "    batch_true_labels = [sample['messages'][2]['content'] for sample in batch_samples]\n",
    "    \n",
    "    # 배치 예측\n",
    "    batch_predictions = predict_batch_with_model(constrained_llm, batch_image_paths, system_prompt)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    for true_label, predicted_label in zip(batch_true_labels, batch_predictions):\n",
    "        if predicted_label == true_label:\n",
    "            correct_baseline += 1\n",
    "    \n",
    "baseline_accuracy = correct_baseline / total_samples\n",
    "print(f\"\\n원본 모델 최종 정확도: {baseline_accuracy:.4f} ({correct_baseline}/{total_samples})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab20c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 모델 평가\n",
    "print(\"\\n=== LoRA 모델 (image_classification) 평가 ===\")\n",
    "\n",
    "# LoRA 모델 설정\n",
    "constrained_lora_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    model=\"image_classification\",  # LoRA 모델 이름\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_choice\": options\n",
    "    }\n",
    ")\n",
    "\n",
    "correct_lora = 0\n",
    "\n",
    "# 배치 단위로 처리\n",
    "for batch_start in tqdm(range(0, total_samples, batch_size), desc=\"LoRA 모델 평가 (배치)\"):\n",
    "    batch_end = min(batch_start + batch_size, total_samples)\n",
    "    batch_samples = test_data[batch_start:batch_end]\n",
    "    \n",
    "    # 배치 데이터 준비\n",
    "    batch_image_paths = [sample['images'][0] for sample in batch_samples]\n",
    "    batch_true_labels = [sample['messages'][2]['content'] for sample in batch_samples]\n",
    "    \n",
    "    # 배치 예측\n",
    "    batch_predictions = predict_batch_with_model(constrained_lora_llm, batch_image_paths, system_prompt)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    for true_label, predicted_label in zip(batch_true_labels, batch_predictions):\n",
    "        if predicted_label == true_label:\n",
    "            correct_lora += 1\n",
    "    \n",
    "lora_accuracy = correct_lora / total_samples\n",
    "print(f\"\\nLoRA 모델 최종 정확도: {lora_accuracy:.4f} ({correct_lora}/{total_samples})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 비교\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 최종 평가 결과\")\n",
    "print(\"=\"*50)\n",
    "print(f\"🔹 원본 모델 정확도:  {baseline_accuracy:.4f} ({correct_baseline}/{total_samples})\")\n",
    "print(f\"🔸 LoRA 모델 정확도:  {lora_accuracy:.4f} ({correct_lora}/{total_samples})\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 성능 향상도 계산\n",
    "improvement = lora_accuracy - baseline_accuracy\n",
    "improvement_percent = (improvement / baseline_accuracy) * 100 if baseline_accuracy > 0 else 0\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"✅ LoRA 모델이 {improvement:.4f} ({improvement_percent:.2f}%) 향상됨\")\n",
    "elif improvement < 0:\n",
    "    print(f\"❌ LoRA 모델이 {abs(improvement):.4f} ({abs(improvement_percent):.2f}%) 하락함\")\n",
    "else:\n",
    "    print(\"🟰 두 모델의 성능이 동일함\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afda3f",
   "metadata": {},
   "source": [
    "## 6. 결론 및 요약\n",
    "\n",
    "### 🎯 실습 결과 정리\n",
    "\n",
    "이번 튜토리얼에서는 다음과 같은 과정을 통해 멀티모달 LLM의 이미지 분류 성능을 향상시켰습니다:\n",
    "\n",
    "1. **🚀 vLLM 서버 설정**: Qwen2.5-VL-7B-Instruct 모델을 멀티모달 서빙 환경에서 실행\n",
    "2. **🧪 Zero-shot 평가**: Constrained decoding을 통해 기본 모델의 분류 성능 확인\n",
    "3. **🛠️ 데이터 준비**: Intel 이미지 분류 데이터셋을 LLaMA-Factory 형식으로 변환\n",
    "4. **📈 Fine-tuning**: LoRA를 사용한 효율적인 모델 학습\n",
    "5. **📊 성능 비교**: Base model vs Fine-tuned model의 정량적 평가\n",
    "\n",
    "### 💡 주요 학습 포인트\n",
    "\n",
    "- **Constrained Decoding**: 출력을 특정 선택지로 제한하여 일관된 분류 결과 확보\n",
    "- **멀티모달 Fine-tuning**: 텍스트와 이미지를 동시에 처리하는 모델의 특화 학습\n",
    "- **LoRA 적용**: 메모리 효율적인 fine-tuning으로 대용량 모델 개선\n",
    "- **배치 평가**: 효율적인 대량 데이터 처리 및 성능 측정\n",
    "\n",
    "### 🔧 실무 활용 방안\n",
    "\n",
    "이 방법론은 다음과 같은 실무 문제에 적용할 수 있습니다:\n",
    "- **의료 이미지 분류**: 엑스레이, MRI 등의 의료 영상 진단 보조\n",
    "- **품질 검사**: 제조업에서의 불량품 자동 분류\n",
    "- **콘텐츠 모더레이션**: 소셜 미디어 이미지 자동 필터링\n",
    "- **문서 분류**: 스캔된 문서의 자동 카테고리 분류"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
