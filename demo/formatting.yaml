### model
model_name_or_path: meta-llama/Llama-3.1-8B-Instruct    # Fine-tuning할 모델
# quantization_bit: 4                                   # 양자화 비트수. 여기선 양자화하지 않음
trust_remote_code: true

### method
stage: sft                                              # Assistant 부분에 대해서 SFT (Supervised Fine-Tuning) 진행
do_train: true
finetuning_type: lora                                   # Full fine-tuning 대신 LoRA 학습
lora_rank: 8
lora_target: all

### dataset
dataset_dir: data
dataset: formatting                                     # data/dataset_info.json 내의 formatting 설정 사용
template: llama3                                        # Llama-3의 채팅에 맞춰 포메팅
cutoff_len: 32768                                       # 최대 Sample 토큰 길이
# max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: save/llama-lora                             # 저장 위치
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: true
report_to: none                                         # choices: [none, wandb, tensorboard, swanlab, mlflow]

### train
# 수학적 배치 크기 계산법: per_device_train_batch_size * gpu_개수 * gradient_accumulation_steps
# Step 수: num_train_epochs * 전체 샘플 수 / 수학적_배치_크기
# Step 수는 100 이상, 수학적 배치는 16 이상으로 잡는것을 권장, 수학적 배치는 2의 제곱수로 할 것.
per_device_train_batch_size: 1                          # gpu 메모리 공간이 남으면 1 이상으로 크기 늘이기. 이때 수학적 배치는 고정할 것!
gradient_accumulation_steps: 32                         # 수학적 배치를 고정하기 위해 수정하는 값
learning_rate: 1.0e-4                                   # 학습률
num_train_epochs: 3.0                                   # Epoch 횟수, Step 수가 적다면 최후로 건들이는 값
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null