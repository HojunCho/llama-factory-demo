{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7dd199",
   "metadata": {},
   "source": [
    "# HTML-to-JSON êµ¬ì¡°í™” ë°ì´í„° ë³€í™˜ íŠœí† ë¦¬ì–¼\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **vLLM**ê³¼ **LLaMA-Factory**ë¥¼ í™œìš©í•˜ì—¬ HTML ì½˜í…ì¸ ë¥¼ êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## 1. vLLM ì„œë²„ ì‹¤í–‰ ë° ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ë¨¼ì € vLLMì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì‚¬ì „ ì¤€ë¹„ì‚¬í•­\n",
    "1. ğŸ¤— Hugging Faceì— ê°€ì…í•˜ì—¬ ì•¡ì„¸ìŠ¤ í† í°ì„ ë°œê¸‰ë°›ìœ¼ì„¸ìš”\n",
    "2. [Llama-3.1-8B-Instruct í˜ì´ì§€](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)ì—ì„œ ëª¨ë¸ ì‚¬ìš© ë“±ë¡ì„ ì™„ë£Œí•˜ì„¸ìš”\n",
    "\n",
    "### vLLM ì„œë²„ ì‹¤í–‰ ëª…ë ¹ì–´ ì„¤ëª…\n",
    "- `HF_TOKEN`: ë°œê¸‰ë°›ì€ Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš”\n",
    "- `CUDA_VISIBLE_DEVICES`: ì‚¬ìš©í•  GPU ë²ˆí˜¸ (ì „ì²´ GPU ì‚¬ìš©ì‹œ ìƒëµ ê°€ëŠ¥)  \n",
    "- `-tp`: í…ì„œ ë³‘ë ¬í™”ì— ì‚¬ìš©í•  GPU ê°œìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤\n",
    "\n",
    "**âš ï¸ ì¤‘ìš”**: ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ë³„ë„ì˜ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38235be3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_TOKEN=<í—ˆê¹…í˜ì´ìŠ¤ í† í°>\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve meta-llama/Llama-3.1-8B-Instruct -tp 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d3489",
   "metadata": {},
   "source": [
    "### âœ… vLLM ì„œë²„ ì‹¤í–‰ í™•ì¸\n",
    "\n",
    "í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë¡œê·¸ê°€ ì¶œë ¥ë˜ë©´ vLLM ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ëœ ê²ƒì…ë‹ˆë‹¤:\n",
    "\n",
    "```log\n",
    "INFO:     Started server process [2609659]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```\n",
    "\n",
    "ì´ì œ **Langchain**ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ì¤‘ì¸ vLLM ì„œë²„ì— ê°„ë‹¨í•œ ìš”ì²­ì„ ë³´ë‚´ ì •ìƒ ì‘ë™ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1188429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain-openai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM ì„œë²„ì— ì—°ê²° (OpenAI í˜¸í™˜ API ì‚¬ìš©)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM ì„œë²„ ì£¼ì†Œ\n",
    "    api_key=\"EMPTY\",  # vLLMì—ì„œëŠ” ì„ì˜ì˜ ê°’ ì‚¬ìš©\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ì±„íŒ… ë©”ì‹œì§€ ë³´ë‚´ê¸°\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(content=\"ì•ˆë…• ë„ˆëŠ” ëˆ„êµ¬ì•¼?\")]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"AI ì‘ë‹µ:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c16cd2",
   "metadata": {},
   "source": [
    "### ğŸ“Š ì„œë²„ ìš”ì²­ ì²˜ë¦¬ í™•ì¸\n",
    "\n",
    "ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•œ í›„, vLLMì„ ì‹¤í–‰ì‹œí‚¨ í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë¡œê·¸ê°€ ì¶œë ¥ë˜ë©´ ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²ƒì…ë‹ˆë‹¤:\n",
    "\n",
    "```log\n",
    "INFO 08-13 23:26:51 [logger.py:41] Received request chatcmpl-543715af49a8443da65b81033b0cd2de: \n",
    "prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>...\n",
    "INFO 08-13 23:26:51 [async_llm.py:269] Added request chatcmpl-543715af49a8443da65b81033b0cd2de.\n",
    "INFO:     127.0.0.1:39490 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
    "INFO 08-13 23:26:56 [loggers.py:122] Engine 000: Avg prompt throughput: 4.2 tokens/s, \n",
    "Avg generation throughput: 4.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, \n",
    "GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
    "```\n",
    "\n",
    "ì´ ë¡œê·¸ë¥¼ í†µí•´ ìš”ì²­ ì²˜ë¦¬ ì†ë„, GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e990364",
   "metadata": {},
   "source": [
    "## 2. ì‹¤ìŠµ ë°ì´í„°ì…‹ ë° ëª©í‘œ ì„¤ì •\n",
    "\n",
    "### ğŸ“‹ ì‚¬ìš© ë°ì´í„°ì…‹\n",
    "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” [mdhasnainali/job-html-to-json](https://huggingface.co/datasets/mdhasnainali/job-html-to-json) ë°ì´í„°ì…‹ì„ í™œìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ¯ ì‹¤ìŠµ ëª©í‘œ\n",
    "HTML í˜•íƒœì˜ ì±„ìš©ê³µê³  ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
    "\n",
    "**ë³€í™˜ ì˜ˆì‹œ:**\n",
    "- **ì…ë ¥**: `data/formatting/input.html` (HTML ì±„ìš©ê³µê³ )\n",
    "- **ì¶œë ¥**: `data/formatting/target.json` (êµ¬ì¡°í™”ëœ JSON)\n",
    "\n",
    "ë¨¼ì € ì…ë ¥ ë°ì´í„°ì™€ ëª©í‘œ ì¶œë ¥ í˜•íƒœë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4039765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.html íŒŒì¼ ë‚´ìš© ì¶œë ¥\n",
    "html_file_path = \"data/formatting/input.html\"\n",
    "with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "    html_content = f.read()\n",
    "print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c00fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.json íŒŒì¼ ë‚´ìš© ì¶œë ¥\n",
    "import json\n",
    "\n",
    "json_file_path = \"data/formatting/target.json\"\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    json_content = f.read()\n",
    "print(json_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0ba58",
   "metadata": {},
   "source": [
    "### ğŸ“ ëª©í‘œ JSON êµ¬ì¡° ì •ì˜\n",
    "\n",
    "ìš°ë¦¬ê°€ ìƒì„±í•˜ê³ ì í•˜ëŠ” JSONì˜ êµ¬ì¡°ë¥¼ **Pydantic**ì„ ì‚¬ìš©í•˜ì—¬ ì •ì˜í•©ë‹ˆë‹¤. ì´ ìŠ¤í‚¤ë§ˆëŠ” ì±„ìš©ê³µê³ ì˜ ëª¨ë“  í•µì‹¬ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ì½”ë“œëŠ” ì±„ìš©ê³µê³  JSONì˜ ìƒì„¸í•œ ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class ApplicationInfo(BaseModel):\n",
    "    apply_url: str\n",
    "    contact_email: str\n",
    "    deadline: str\n",
    "\n",
    "\n",
    "class Location(BaseModel):\n",
    "    city: str\n",
    "    state: str | None\n",
    "    country: str | None\n",
    "    remote: bool | None\n",
    "    hybrid: bool | None\n",
    "\n",
    "\n",
    "class Qualifications(BaseModel):\n",
    "    education_level: str | None\n",
    "    fields_of_study: str | None\n",
    "    certifications: list[str] | None\n",
    "\n",
    "\n",
    "class Salary(BaseModel):\n",
    "    currency: str | None\n",
    "    min: float | None\n",
    "    max: float | None\n",
    "    period: str | None\n",
    "\n",
    "\n",
    "class YearsOfExperience(BaseModel):\n",
    "    min: float | None\n",
    "    max: float | None\n",
    "\n",
    "\n",
    "class JobPosting(BaseModel):\n",
    "    job_id: str\n",
    "    title: str\n",
    "    department: str\n",
    "    employment_type: str\n",
    "    experience_level: str\n",
    "    posted_date: str\n",
    "    work_schedule: str\n",
    "\n",
    "    location: Location\n",
    "    application_info: ApplicationInfo\n",
    "    salary: Salary\n",
    "    years_of_experience: YearsOfExperience\n",
    "\n",
    "    requirements: list[str]\n",
    "    responsibilities: list[str]\n",
    "    nice_to_have: list[str]\n",
    "    qualifications: Qualifications\n",
    "    recruitment_process: list[str]\n",
    "    programming_languages: list[str]\n",
    "    tools: list[str]\n",
    "    databases: list[str]\n",
    "    cloud_providers: list[str]\n",
    "    language_requirements: list[str]\n",
    "    benefits: list[str]\n",
    "\n",
    "print(json.dumps(JobPosting.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b951f",
   "metadata": {},
   "source": [
    "## 3. Zero-shot ì •í™•ë„ í‰ê°€\n",
    "\n",
    "### ğŸ§ª ì‹¤í—˜ 1: ê¸°ë³¸ Zero-shot ì˜ˆì¸¡\n",
    "\n",
    "ë¨¼ì € **fine-tuning ì—†ì´** ê¸°ë³¸ LLaMA ëª¨ë¸ì´ HTMLì„ JSONìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ ë³€í™˜í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b86d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot HTML to JSON ë³€í™˜\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "# JSON schema ê°€ì ¸ì˜¤ê¸°\n",
    "json_schema = json.dumps(JobPosting.model_json_schema(), indent=2)\n",
    "\n",
    "# System prompt ìƒì„±\n",
    "system_prompt = f\"\"\"ì£¼ì–´ì§„ HTML ì…ë ¥ì„ JSONìœ¼ë¡œ ë°”ê¾¸ì‹œì˜¤. ì´ë•Œ JSON schemaëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:\n",
    "{json_schema}\n",
    "\n",
    "HTMLì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” JSON í˜•íƒœë¡œ ë³€í™˜í•˜ì‹œì˜¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ê³  jsonì„ ë°”ë¡œ ì¶œë ¥í•˜ì‹œì˜¤.\"\"\"\n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=html_content)\n",
    "]\n",
    "\n",
    "# LLMì— ìš”ì²­\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fb64d",
   "metadata": {},
   "source": [
    "### ğŸ”§ ì‹¤í—˜ 2: Constrained Decodingìœ¼ë¡œ Zero-shot ì˜ˆì¸¡\n",
    "\n",
    "ê¸°ë³¸ zero-shot ì¶œë ¥ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "- ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ê°€ í¬í•¨ë¨\n",
    "- JSON êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ\n",
    "- ìŠ¤í‚¤ë§ˆë¥¼ ì¤€ìˆ˜í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Constrained Decoding**ì„ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ì„ JSON ìŠ¤í‚¤ë§ˆì— ê°•ì œë¡œ ë§ì¶°ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf125e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot HTML to JSON ë³€í™˜\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "# JSON schema ê°€ì ¸ì˜¤ê¸°\n",
    "json_schema = json.dumps(JobPosting.model_json_schema(), indent=2)\n",
    "\n",
    "# System prompt ìƒì„±\n",
    "system_prompt = f\"\"\"ì£¼ì–´ì§„ HTML ì…ë ¥ì„ JSONìœ¼ë¡œ ë°”ê¾¸ì‹œì˜¤. ì´ë•Œ JSON schemaëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:\n",
    "{json_schema}\n",
    "\n",
    "HTMLì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” JSON í˜•íƒœë¡œ ë³€í™˜í•˜ì‹œì˜¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ê³  jsonì„ ë°”ë¡œ ì¶œë ¥í•˜ì‹œì˜¤.\"\"\"\n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=html_content)\n",
    "]\n",
    "\n",
    "# Constrained Decodingì´ ì ìš©\n",
    "constrained_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM ì„œë²„ ì£¼ì†Œ\n",
    "    api_key=\"EMPTY\",  # vLLMì—ì„œëŠ” ì„ì˜ì˜ ê°’ ì‚¬ìš©\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_json\": JobPosting.model_json_schema() \n",
    "    }\n",
    ")\n",
    "\n",
    "# LLMì— ìš”ì²­\n",
    "response = constrained_llm.invoke(messages)\n",
    "\n",
    "obj = json.loads(response.content)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb60c5a",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Zero-shot ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "Constrained decodingì„ í†µí•´ JSON í˜•íƒœì˜ ì¶œë ¥ì„ ì–»ì„ ìˆ˜ ìˆì—ˆì§€ë§Œ, ì‹¤ì œ ì •ë‹µ(ground truth)ê³¼ ì„¸ë¶€ì ìœ¼ë¡œ ë¹„êµí•´ë³´ë©´ ì •í™•ë„ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì´ì œ **Fine-tuning**ì„ í†µí•´ ì„±ëŠ¥ì„ ê°œì„ í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fb0b0",
   "metadata": {},
   "source": [
    "## 4. LLaMA-Factoryë¥¼ í™œìš©í•œ Fine-tuning\n",
    "\n",
    "### ğŸ› ï¸ í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "ì´ì œ **LLaMA-Factory**ë¥¼ ì‚¬ìš©í•˜ì—¬ HTML-to-JSON ë³€í™˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ fine-tuningì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ í•™ìŠµìš© ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³  ìë£Œ**: [LLaMA-Factory ë°ì´í„°ì…‹ ì„¤ì • ê°€ì´ë“œ](https://github.com/hiyouga/LLaMA-Factory/tree/main/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fa467",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ìƒì„± ì½”ë“œ\n",
    "python -m demo.create_formatting_dataset\n",
    "# ìƒì„±ëœ ë°ì´í„° í™•ì¸\n",
    "head -16 data/formatting/train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506773a",
   "metadata": {},
   "source": [
    "### ğŸ“Š ë°ì´í„°ì…‹ í† í° ê¸¸ì´ ë¶„ì„\n",
    "\n",
    "Fine-tuningì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ìƒì„±ëœ ë°ì´í„°ì…‹ì˜ í† í° ê¸¸ì´ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì ì ˆí•œ `cutoff_len` ì„¤ì •ì„ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer ë¡œë“œ ë° í† í° ê¸¸ì´ ì¸¡ì • (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Llama-3.1-8B-Instruct tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "with open(\"data/formatting/train.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"ì´ ë°ì´í„° ê°œìˆ˜: {len(data)}\")\n",
    "\n",
    "# ëª¨ë“  ë©”ì‹œì§€ë¥¼ chat templateìœ¼ë¡œ ë³€í™˜\n",
    "print(\"Chat template ì ìš© ì¤‘...\")\n",
    "formatted_texts = []\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    messages = item[\"messages\"]\n",
    "    \n",
    "    # apply_chat_template ì‚¬ìš©\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    formatted_texts.append(formatted_text)\n",
    "    \n",
    "    # ì§„í–‰ìƒí™© ì¶œë ¥\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"ì§„í–‰: {i + 1}/{len(data)}\")\n",
    "\n",
    "print(\"ë°°ì¹˜ í† í°í™” ì‹œì‘...\")\n",
    "\n",
    "# ë°°ì¹˜ í† í°í™” (ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ)\n",
    "batch_size = 100\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(0, len(formatted_texts), batch_size):\n",
    "    batch_texts = formatted_texts[i:i + batch_size]\n",
    "    \n",
    "    # ë°°ì¹˜ í† í°í™”\n",
    "    batch_tokens = tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False,\n",
    "        padding=False,\n",
    "        truncation=False\n",
    "    )\n",
    "    \n",
    "    # ê° í…ìŠ¤íŠ¸ì˜ í† í° ê¸¸ì´ ê³„ì‚°\n",
    "    for tokens in batch_tokens['input_ids']:\n",
    "        token_lengths.append(len(tokens))\n",
    "    \n",
    "    # ì§„í–‰ìƒí™© ì¶œë ¥\n",
    "    if (i + batch_size) % 1000 == 0:\n",
    "        print(f\"í† í°í™” ì§„í–‰: {min(i + batch_size, len(formatted_texts))}/{len(formatted_texts)}\")\n",
    "\n",
    "print(f\"í† í°í™” ì™„ë£Œ! ì´ {len(token_lengths)}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ë¨\")\n",
    "\n",
    "# í†µê³„ ê³„ì‚°\n",
    "token_lengths = np.array(token_lengths)\n",
    "print(f\"\\n=== í† í° ê¸¸ì´ í†µê³„ ===\")\n",
    "print(f\"í‰ê·  í† í° ê¸¸ì´: {np.mean(token_lengths):.2f}\")\n",
    "print(f\"ì¤‘ê°„ê°’ í† í° ê¸¸ì´: {np.median(token_lengths):.2f}\")\n",
    "print(f\"ìµœì†Œ í† í° ê¸¸ì´: {np.min(token_lengths)}\")\n",
    "print(f\"ìµœëŒ€ í† í° ê¸¸ì´: {np.max(token_lengths)}\")\n",
    "print(f\"í‘œì¤€í¸ì°¨: {np.std(token_lengths):.2f}\")\n",
    "\n",
    "# ë¶„ìœ„ìˆ˜ ì •ë³´\n",
    "print(f\"\\n=== í† í° ê¸¸ì´ ë¶„ìœ„ìˆ˜ ===\")\n",
    "print(f\"25% ë¶„ìœ„ìˆ˜: {np.percentile(token_lengths, 25):.2f}\")\n",
    "print(f\"50% ë¶„ìœ„ìˆ˜ (ì¤‘ê°„ê°’): {np.percentile(token_lengths, 50):.2f}\")\n",
    "print(f\"75% ë¶„ìœ„ìˆ˜: {np.percentile(token_lengths, 75):.2f}\")\n",
    "print(f\"90% ë¶„ìœ„ìˆ˜: {np.percentile(token_lengths, 90):.2f}\")\n",
    "print(f\"95% ë¶„ìœ„ìˆ˜: {np.percentile(token_lengths, 95):.2f}\")\n",
    "print(f\"99% ë¶„ìœ„ìˆ˜: {np.percentile(token_lengths, 99):.2f}\")\n",
    "\n",
    "# í† í° ê¸¸ì´ë³„ ë¶„í¬\n",
    "print(f\"\\n=== í† í° ê¸¸ì´ ë¶„í¬ ===\")\n",
    "bins = [0, 1000, 2000, 4000, 8000, 16000, 32000, float('inf')]\n",
    "labels = ['0-1K', '1K-2K', '2K-4K', '4K-8K', '8K-16K', '16K-32K', '32K+']\n",
    "\n",
    "for i in range(len(bins)-1):\n",
    "    count = np.sum((token_lengths >= bins[i]) & (token_lengths < bins[i+1]))\n",
    "    percentage = count / len(token_lengths) * 100\n",
    "    print(f\"{labels[i]}: {count}ê°œ ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24d94d",
   "metadata": {},
   "source": [
    "### âš™ï¸ í•™ìŠµ ì„¤ì • ê²°ì •\n",
    "\n",
    "ë¶„ì„ ê²°ê³¼, 32K í† í°ì„ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œì´ ì—†ìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ `cutoff_len=32768`ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ fine-tuningì„ ì‹œì‘í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030892e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# demo/formatting.yaml íŒŒì¼ì„ í™•ì¸í•´ë³´ì.\n",
    "CUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train demo/formatting.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f22d7",
   "metadata": {},
   "source": [
    "### ğŸš€ Fine-tuned ëª¨ë¸(LoRA) ì„œë¹™\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´ vLLMì— **LoRA ì–´ëŒ‘í„°**ë¥¼ ì¶”ê°€í•˜ì—¬ fine-tuned ëª¨ë¸ì„ ì„œë¹™í•©ë‹ˆë‹¤. \n",
    "\n",
    "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ LoRAê°€ ì ìš©ëœ vLLM ì„œë²„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47ee5e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_TOKEN=<í—ˆê¹…í˜ì´ìŠ¤ í† í°>\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve meta-llama/Llama-3.1-8B-Instruct -tp 2 --enable-lora --lora-modules formatting=$PWD/save/llama-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f9e62",
   "metadata": {},
   "source": [
    "## 5. ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€\n",
    "\n",
    "### ğŸ” Base Model vs Fine-tuned Model ë¹„êµ\n",
    "\n",
    "vLLM ì„œë²„ê°€ LoRAì™€ í•¨ê»˜ ì‹¤í–‰ë˜ë©´, ë‹¤ìŒ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì§ì ‘ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **Base Model**: ì›ë³¸ Llama-3.1-8B-Instruct\n",
    "2. **Fine-tuned Model**: LoRAê°€ ì ìš©ëœ ëª¨ë¸\n",
    "\n",
    "ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•œ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ì„ ë¹„êµí•˜ì—¬ fine-tuningì˜ íš¨ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a43870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "# JSON schema ê°€ì ¸ì˜¤ê¸°\n",
    "json_schema = json.dumps(JobPosting.model_json_schema(), indent=2)\n",
    "\n",
    "# System prompt ìƒì„±\n",
    "system_prompt = f\"\"\"ì£¼ì–´ì§„ HTML ì…ë ¥ì„ JSONìœ¼ë¡œ ë°”ê¾¸ì‹œì˜¤. ì´ë•Œ JSON schemaëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:\n",
    "{json_schema}\n",
    "\n",
    "HTMLì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” JSON í˜•íƒœë¡œ ë³€í™˜í•˜ì‹œì˜¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ê³  jsonì„ ë°”ë¡œ ì¶œë ¥í•˜ì‹œì˜¤.\"\"\"\n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=html_content)\n",
    "]\n",
    "\n",
    "# ê¸°ì¡´ LLM\n",
    "constrained_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM ì„œë²„ ì£¼ì†Œ\n",
    "    api_key=\"EMPTY\",  # vLLMì—ì„œëŠ” ì„ì˜ì˜ ê°’ ì‚¬ìš©\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_json\": JobPosting.model_json_schema() \n",
    "    }\n",
    ")\n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=\"\".join(html_content.split()))          # LoRAì— ì‚¬ìš©ëœ í•™ìŠµë°ì´í„°ëŠ”, í•™ìŠµ ë°ì´í„°ì— ì¤„ ë‚˜ëˆ”ì´ ì—†ë‹¤.\n",
    "]\n",
    "\n",
    "# LoRAê°€ ì ìš©ëœ LLM\n",
    "constrained_lora_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM ì„œë²„ ì£¼ì†Œ\n",
    "    api_key=\"EMPTY\",  # vLLMì—ì„œëŠ” ì„ì˜ì˜ ê°’ ì‚¬ìš©\n",
    "    model=\"formatting\",  # LoRA ë²„ì „\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_json\": JobPosting.model_json_schema() \n",
    "    }\n",
    ")\n",
    "\n",
    "import pprint\n",
    "# ê·¸ëƒ¥ LLMì— ìš”ì²­\n",
    "response = constrained_llm.invoke(messages)\n",
    "\n",
    "naive_output = json.loads(response.content)\n",
    "\n",
    "print(\"ê¸°ì¡´ LLM ì‘ë‹µ:\")\n",
    "pprint.pprint(naive_output)\n",
    "\n",
    "\n",
    "# Lora LLMì— ìš”ì²­\n",
    "response = constrained_lora_llm.invoke(messages)\n",
    "\n",
    "lora_output = json.loads(response.content)\n",
    "\n",
    "print(\"LoRA LLM ì‘ë‹µ:\")\n",
    "pprint.pprint(lora_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28206e1c",
   "metadata": {},
   "source": [
    "### ğŸ“ ì •ëŸ‰ì  ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "ë‘ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ê°„ë‹¨í•˜ê²Œ ë¹„êµí•˜ê¸° ìœ„í•´ **í…ìŠ¤íŠ¸ ìœ ì‚¬ì„±**ì„ ì¸¡ì •í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "**í‰ê°€ ë°©ë²•:**\n",
    "- ì •ë‹µ JSONê³¼ ê° ëª¨ë¸ì˜ ì¶œë ¥ JSONì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "- `difflib.SequenceMatcher`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ì„± ì ìˆ˜ ê³„ì‚°\n",
    "- 0.0 (ì™„ì „íˆ ë‹¤ë¦„) ~ 1.0 (ì™„ì „íˆ ê°™ìŒ) ë²”ìœ„ë¡œ í‰ê°€\n",
    "\n",
    "ì´ë¥¼ í†µí•´ fine-tuningì´ ì‹¤ì œë¡œ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í–ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa69f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ìœ ì‚¬ì„± ë¹„êµ\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# JSONì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ìœ ì‚¬ì„± ì¸¡ì •\n",
    "target_str = json.dumps(json.loads(json_content), sort_keys=True, ensure_ascii=False)\n",
    "lora_str = json.dumps(lora_output, sort_keys=True, ensure_ascii=False)\n",
    "naive_str = json.dumps(naive_output, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "# ìœ ì‚¬ì„± ê³„ì‚°\n",
    "lora_similarity = SequenceMatcher(None, target_str, lora_str).ratio()\n",
    "naive_similarity = SequenceMatcher(None, target_str, naive_str).ratio()\n",
    "\n",
    "print(\"=== ìœ ì‚¬ì„± ë¹„êµ ê²°ê³¼ ===\")\n",
    "print(f\"LoRA ëª¨ë¸ ìœ ì‚¬ì„±:  {lora_similarity:.4f}\")\n",
    "print(f\"Naive ëª¨ë¸ ìœ ì‚¬ì„±: {naive_similarity:.4f}\")\n",
    "print(f\"ì°¨ì´: {lora_similarity - naive_similarity:.4f}\")\n",
    "\n",
    "if lora_similarity > naive_similarity:\n",
    "    improvement = ((lora_similarity - naive_similarity) / naive_similarity) * 100\n",
    "    print(f\"âœ… LoRA ëª¨ë¸ì´ {improvement:.2f}% ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    decline = ((naive_similarity - lora_similarity) / naive_similarity) * 100\n",
    "    print(f\"âŒ Naive ëª¨ë¸ì´ {decline:.2f}% ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
