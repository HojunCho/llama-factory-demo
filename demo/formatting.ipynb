{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7dd199",
   "metadata": {},
   "source": [
    "# HTML-to-JSON 구조화 데이터 변환 튜토리얼\n",
    "\n",
    "이 튜토리얼에서는 **vLLM**과 **LLaMA-Factory**를 활용하여 HTML 콘텐츠를 구조화된 JSON 형태로 변환하는 방법을 학습합니다.\n",
    "\n",
    "## 1. vLLM 서버 실행 및 연결 테스트\n",
    "\n",
    "먼저 vLLM이 정상적으로 작동하는지 확인해보겠습니다. 우리는 [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) 모델을 사용합니다.\n",
    "\n",
    "### 사전 준비사항\n",
    "1. 🤗 Hugging Face에 가입하여 액세스 토큰을 발급받으세요\n",
    "2. [Llama-3.1-8B-Instruct 페이지](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)에서 모델 사용 등록을 완료하세요\n",
    "\n",
    "### vLLM 서버 실행 명령어 설명\n",
    "- `HF_TOKEN`: 발급받은 Hugging Face 토큰을 입력하세요\n",
    "- `CUDA_VISIBLE_DEVICES`: 사용할 GPU 번호 (전체 GPU 사용시 생략 가능)  \n",
    "- `-tp`: 텐서 병렬화에 사용할 GPU 개수를 지정합니다\n",
    "\n",
    "**⚠️ 중요**: 아래 명령어를 별도의 터미널에서 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38235be3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_TOKEN=<허깅페이스 토큰>\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve meta-llama/Llama-3.1-8B-Instruct -tp 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d3489",
   "metadata": {},
   "source": [
    "### ✅ vLLM 서버 실행 확인\n",
    "\n",
    "터미널에서 다음과 같은 로그가 출력되면 vLLM 서버가 정상적으로 시작된 것입니다:\n",
    "\n",
    "```log\n",
    "INFO:     Started server process [2609659]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```\n",
    "\n",
    "이제 **Langchain**을 사용하여 실행 중인 vLLM 서버에 간단한 요청을 보내 정상 작동을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1188429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain-openai 라이브러리 임포트\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM 서버에 연결 (OpenAI 호환 API 사용)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM 서버 주소\n",
    "    api_key=\"EMPTY\",  # vLLM에서는 임의의 값 사용\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # 모델 이름\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 채팅 메시지 보내기\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(content=\"안녕 너는 누구야?\")]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"AI 응답:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c16cd2",
   "metadata": {},
   "source": [
    "### 📊 서버 요청 처리 확인\n",
    "\n",
    "위 코드를 실행한 후, vLLM을 실행시킨 터미널에서 다음과 같은 로그가 출력되면 요청이 성공적으로 처리된 것입니다:\n",
    "\n",
    "```log\n",
    "INFO 08-13 23:26:51 [logger.py:41] Received request chatcmpl-543715af49a8443da65b81033b0cd2de: \n",
    "prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>...\n",
    "INFO 08-13 23:26:51 [async_llm.py:269] Added request chatcmpl-543715af49a8443da65b81033b0cd2de.\n",
    "INFO:     127.0.0.1:39490 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
    "INFO 08-13 23:26:56 [loggers.py:122] Engine 000: Avg prompt throughput: 4.2 tokens/s, \n",
    "Avg generation throughput: 4.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, \n",
    "GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
    "```\n",
    "\n",
    "이 로그를 통해 요청 처리 속도, GPU 메모리 사용량 등을 모니터링할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e990364",
   "metadata": {},
   "source": [
    "## 2. 실습 데이터셋 및 목표 설정\n",
    "\n",
    "### 📋 사용 데이터셋\n",
    "이번 실습에서는 [mdhasnainali/job-html-to-json](https://huggingface.co/datasets/mdhasnainali/job-html-to-json) 데이터셋을 활용합니다.\n",
    "\n",
    "### 🎯 실습 목표\n",
    "HTML 형태의 채용공고 데이터를 구조화된 JSON 형태로 변환하는 것이 목표입니다.\n",
    "\n",
    "**변환 예시:**\n",
    "- **입력**: `data/formatting/input.html` (HTML 채용공고)\n",
    "- **출력**: `data/formatting/target.json` (구조화된 JSON)\n",
    "\n",
    "먼저 입력 데이터와 목표 출력 형태를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4039765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.html 파일 내용 출력\n",
    "html_file_path = \"data/formatting/input.html\"\n",
    "with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "    html_content = f.read()\n",
    "print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c00fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.json 파일 내용 출력\n",
    "import json\n",
    "\n",
    "json_file_path = \"data/formatting/target.json\"\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    json_content = f.read()\n",
    "print(json_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0ba58",
   "metadata": {},
   "source": [
    "### 📝 목표 JSON 구조 정의\n",
    "\n",
    "우리가 생성하고자 하는 JSON의 구조를 **Pydantic**을 사용하여 정의합니다. 이 스키마는 채용공고의 모든 핵심 정보를 체계적으로 구조화합니다.\n",
    "\n",
    "다음 코드는 채용공고 JSON의 상세한 스키마를 정의합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class ApplicationInfo(BaseModel):\n",
    "    apply_url: str\n",
    "    contact_email: str\n",
    "    deadline: str\n",
    "\n",
    "\n",
    "class Location(BaseModel):\n",
    "    city: str\n",
    "    state: str | None\n",
    "    country: str | None\n",
    "    remote: bool | None\n",
    "    hybrid: bool | None\n",
    "\n",
    "\n",
    "class Qualifications(BaseModel):\n",
    "    education_level: str | None\n",
    "    fields_of_study: str | None\n",
    "    certifications: list[str] | None\n",
    "\n",
    "\n",
    "class Salary(BaseModel):\n",
    "    currency: str | None\n",
    "    min: float | None\n",
    "    max: float | None\n",
    "    period: str | None\n",
    "\n",
    "\n",
    "class YearsOfExperience(BaseModel):\n",
    "    min: float | None\n",
    "    max: float | None\n",
    "\n",
    "\n",
    "class JobPosting(BaseModel):\n",
    "    job_id: str\n",
    "    title: str\n",
    "    department: str\n",
    "    employment_type: str\n",
    "    experience_level: str\n",
    "    posted_date: str\n",
    "    work_schedule: str\n",
    "\n",
    "    location: Location\n",
    "    application_info: ApplicationInfo\n",
    "    salary: Salary\n",
    "    years_of_experience: YearsOfExperience\n",
    "\n",
    "    requirements: list[str]\n",
    "    responsibilities: list[str]\n",
    "    nice_to_have: list[str]\n",
    "    qualifications: Qualifications\n",
    "    recruitment_process: list[str]\n",
    "    programming_languages: list[str]\n",
    "    tools: list[str]\n",
    "    databases: list[str]\n",
    "    cloud_providers: list[str]\n",
    "    language_requirements: list[str]\n",
    "    benefits: list[str]\n",
    "\n",
    "print(json.dumps(JobPosting.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b951f",
   "metadata": {},
   "source": [
    "## 3. Zero-shot 정확도 평가\n",
    "\n",
    "### 🧪 실험 1: 기본 Zero-shot 예측\n",
    "\n",
    "먼저 **fine-tuning 없이** 기본 LLaMA 모델이 HTML을 JSON으로 얼마나 잘 변환할 수 있는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b86d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot HTML to JSON 변환\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "# JSON schema 가져오기\n",
    "json_schema = json.dumps(JobPosting.model_json_schema(), indent=2)\n",
    "\n",
    "# System prompt 생성\n",
    "system_prompt = f\"\"\"주어진 HTML 입력을 JSON으로 바꾸시오. 이때 JSON schema는 다음과 같다:\n",
    "{json_schema}\n",
    "\n",
    "HTML에서 정보를 추출하여 위 스키마에 맞는 JSON 형태로 변환하시오. 다른 텍스트는 포함하지 말고 json을 바로 출력하시오.\"\"\"\n",
    "\n",
    "# 메시지 구성\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=html_content)\n",
    "]\n",
    "\n",
    "# LLM에 요청\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fb64d",
   "metadata": {},
   "source": [
    "### 🔧 실험 2: Constrained Decoding으로 Zero-shot 예측\n",
    "\n",
    "기본 zero-shot 출력에서는 다음과 같은 문제가 발생할 수 있습니다:\n",
    "- 불필요한 텍스트가 포함됨\n",
    "- JSON 구조가 올바르지 않음\n",
    "- 스키마를 준수하지 않음\n",
    "\n",
    "이러한 문제를 해결하기 위해 **Constrained Decoding**을 사용하여 출력을 JSON 스키마에 강제로 맞춰보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf125e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot HTML to JSON 변환\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "# JSON schema 가져오기\n",
    "json_schema = json.dumps(JobPosting.model_json_schema(), indent=2)\n",
    "\n",
    "# System prompt 생성\n",
    "system_prompt = f\"\"\"주어진 HTML 입력을 JSON으로 바꾸시오. 이때 JSON schema는 다음과 같다:\n",
    "{json_schema}\n",
    "\n",
    "HTML에서 정보를 추출하여 위 스키마에 맞는 JSON 형태로 변환하시오. 다른 텍스트는 포함하지 말고 json을 바로 출력하시오.\"\"\"\n",
    "\n",
    "# 메시지 구성\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=html_content)\n",
    "]\n",
    "\n",
    "# Constrained Decoding이 적용\n",
    "constrained_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM 서버 주소\n",
    "    api_key=\"EMPTY\",  # vLLM에서는 임의의 값 사용\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # 모델 이름\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_json\": JobPosting.model_json_schema() \n",
    "    }\n",
    ")\n",
    "\n",
    "# LLM에 요청\n",
    "response = constrained_llm.invoke(messages)\n",
    "\n",
    "obj = json.loads(response.content)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb60c5a",
   "metadata": {},
   "source": [
    "### 📈 Zero-shot 결과 분석\n",
    "\n",
    "Constrained decoding을 통해 JSON 형태의 출력을 얻을 수 있었지만, 실제 정답(ground truth)과 세부적으로 비교해보면 정확도에 한계가 있습니다. \n",
    "\n",
    "이제 **Fine-tuning**을 통해 성능을 개선해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fb0b0",
   "metadata": {},
   "source": [
    "## 4. LLaMA-Factory를 활용한 Fine-tuning\n",
    "\n",
    "### 🛠️ 학습 데이터 준비\n",
    "\n",
    "이제 **LLaMA-Factory**를 사용하여 HTML-to-JSON 변환 성능을 향상시키기 위한 fine-tuning을 진행합니다.\n",
    "\n",
    "먼저 다음 명령어를 실행하여 학습용 데이터셋을 생성하겠습니다.\n",
    "\n",
    "**참고 자료**: [LLaMA-Factory 데이터셋 설정 가이드](https://github.com/hiyouga/LLaMA-Factory/tree/main/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fa467",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 생성 코드\n",
    "python -m demo.create_formatting_dataset\n",
    "# 생성된 데이터 확인\n",
    "head -16 data/formatting/train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506773a",
   "metadata": {},
   "source": [
    "### 📊 데이터셋 토큰 길이 분석\n",
    "\n",
    "Fine-tuning을 효과적으로 수행하기 위해 생성된 데이터셋의 토큰 길이 분포를 분석합니다. 이를 통해 적절한 `cutoff_len` 설정을 결정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로드 및 토큰 길이 측정 (배치 처리)\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Llama-3.1-8B-Instruct tokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# 데이터 로드\n",
    "with open(\"data/formatting/train.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"총 데이터 개수: {len(data)}\")\n",
    "\n",
    "# 모든 메시지를 chat template으로 변환\n",
    "print(\"Chat template 적용 중...\")\n",
    "formatted_texts = []\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    messages = item[\"messages\"]\n",
    "    \n",
    "    # apply_chat_template 사용\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    formatted_texts.append(formatted_text)\n",
    "    \n",
    "    # 진행상황 출력\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"진행: {i + 1}/{len(data)}\")\n",
    "\n",
    "print(\"배치 토큰화 시작...\")\n",
    "\n",
    "# 배치 토큰화 (메모리를 고려하여 배치 크기 조절)\n",
    "batch_size = 100\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(0, len(formatted_texts), batch_size):\n",
    "    batch_texts = formatted_texts[i:i + batch_size]\n",
    "    \n",
    "    # 배치 토큰화\n",
    "    batch_tokens = tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False,\n",
    "        padding=False,\n",
    "        truncation=False\n",
    "    )\n",
    "    \n",
    "    # 각 텍스트의 토큰 길이 계산\n",
    "    for tokens in batch_tokens['input_ids']:\n",
    "        token_lengths.append(len(tokens))\n",
    "    \n",
    "    # 진행상황 출력\n",
    "    if (i + batch_size) % 1000 == 0:\n",
    "        print(f\"토큰화 진행: {min(i + batch_size, len(formatted_texts))}/{len(formatted_texts)}\")\n",
    "\n",
    "print(f\"토큰화 완료! 총 {len(token_lengths)}개 샘플 처리됨\")\n",
    "\n",
    "# 통계 계산\n",
    "token_lengths = np.array(token_lengths)\n",
    "print(f\"\\n=== 토큰 길이 통계 ===\")\n",
    "print(f\"평균 토큰 길이: {np.mean(token_lengths):.2f}\")\n",
    "print(f\"중간값 토큰 길이: {np.median(token_lengths):.2f}\")\n",
    "print(f\"최소 토큰 길이: {np.min(token_lengths)}\")\n",
    "print(f\"최대 토큰 길이: {np.max(token_lengths)}\")\n",
    "print(f\"표준편차: {np.std(token_lengths):.2f}\")\n",
    "\n",
    "# 분위수 정보\n",
    "print(f\"\\n=== 토큰 길이 분위수 ===\")\n",
    "print(f\"25% 분위수: {np.percentile(token_lengths, 25):.2f}\")\n",
    "print(f\"50% 분위수 (중간값): {np.percentile(token_lengths, 50):.2f}\")\n",
    "print(f\"75% 분위수: {np.percentile(token_lengths, 75):.2f}\")\n",
    "print(f\"90% 분위수: {np.percentile(token_lengths, 90):.2f}\")\n",
    "print(f\"95% 분위수: {np.percentile(token_lengths, 95):.2f}\")\n",
    "print(f\"99% 분위수: {np.percentile(token_lengths, 99):.2f}\")\n",
    "\n",
    "# 토큰 길이별 분포\n",
    "print(f\"\\n=== 토큰 길이 분포 ===\")\n",
    "bins = [0, 1000, 2000, 4000, 8000, 16000, 32000, float('inf')]\n",
    "labels = ['0-1K', '1K-2K', '2K-4K', '4K-8K', '8K-16K', '16K-32K', '32K+']\n",
    "\n",
    "for i in range(len(bins)-1):\n",
    "    count = np.sum((token_lengths >= bins[i]) & (token_lengths < bins[i+1]))\n",
    "    percentage = count / len(token_lengths) * 100\n",
    "    print(f\"{labels[i]}: {count}개 ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24d94d",
   "metadata": {},
   "source": [
    "### ⚙️ 학습 설정 결정\n",
    "\n",
    "분석 결과, 32K 토큰을 초과하는 샘플이 없음을 확인했습니다. 따라서 `cutoff_len=32768`로 설정하여 모든 데이터를 효율적으로 활용할 수 있습니다.\n",
    "\n",
    "이제 fine-tuning을 시작합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030892e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# demo/formatting.yaml 파일을 확인해보자.\n",
    "CUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train demo/formatting.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f22d7",
   "metadata": {},
   "source": [
    "### 🚀 Fine-tuned 모델(LoRA) 서빙\n",
    "\n",
    "학습이 완료되면 vLLM에 **LoRA 어댑터**를 추가하여 fine-tuned 모델을 서빙합니다. \n",
    "\n",
    "다음 명령어로 LoRA가 적용된 vLLM 서버를 실행하세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47ee5e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HF_TOKEN=<허깅페이스 토큰>\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve meta-llama/Llama-3.1-8B-Instruct -tp 2 --enable-lora --lora-modules formatting=$PWD/save/llama-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f9e62",
   "metadata": {},
   "source": [
    "## 5. 성능 비교 및 평가\n",
    "\n",
    "### 🔍 Base Model vs Fine-tuned Model 비교\n",
    "\n",
    "vLLM 서버가 LoRA와 함께 실행되면, 다음 두 모델의 성능을 직접 비교해보겠습니다:\n",
    "\n",
    "1. **Base Model**: 원본 Llama-3.1-8B-Instruct\n",
    "2. **Fine-tuned Model**: LoRA가 적용된 모델\n",
    "\n",
    "동일한 입력에 대한 두 모델의 출력을 비교하여 fine-tuning의 효과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a43870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "# JSON schema 가져오기\n",
    "json_schema = json.dumps(JobPosting.model_json_schema(), indent=2)\n",
    "\n",
    "# System prompt 생성\n",
    "system_prompt = f\"\"\"주어진 HTML 입력을 JSON으로 바꾸시오. 이때 JSON schema는 다음과 같다:\n",
    "{json_schema}\n",
    "\n",
    "HTML에서 정보를 추출하여 위 스키마에 맞는 JSON 형태로 변환하시오. 다른 텍스트는 포함하지 말고 json을 바로 출력하시오.\"\"\"\n",
    "\n",
    "# 메시지 구성\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=html_content)\n",
    "]\n",
    "\n",
    "# 기존 LLM\n",
    "constrained_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM 서버 주소\n",
    "    api_key=\"EMPTY\",  # vLLM에서는 임의의 값 사용\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # 모델 이름\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_json\": JobPosting.model_json_schema() \n",
    "    }\n",
    ")\n",
    "\n",
    "# 메시지 구성\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=\"\".join(html_content.split()))          # LoRA에 사용된 학습데이터는, 학습 데이터에 줄 나눔이 없다.\n",
    "]\n",
    "\n",
    "# LoRA가 적용된 LLM\n",
    "constrained_lora_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM 서버 주소\n",
    "    api_key=\"EMPTY\",  # vLLM에서는 임의의 값 사용\n",
    "    model=\"formatting\",  # LoRA 버전\n",
    "    temperature=0.1,\n",
    "    extra_body={\n",
    "        \"guided_json\": JobPosting.model_json_schema() \n",
    "    }\n",
    ")\n",
    "\n",
    "import pprint\n",
    "# 그냥 LLM에 요청\n",
    "response = constrained_llm.invoke(messages)\n",
    "\n",
    "naive_output = json.loads(response.content)\n",
    "\n",
    "print(\"기존 LLM 응답:\")\n",
    "pprint.pprint(naive_output)\n",
    "\n",
    "\n",
    "# Lora LLM에 요청\n",
    "response = constrained_lora_llm.invoke(messages)\n",
    "\n",
    "lora_output = json.loads(response.content)\n",
    "\n",
    "print(\"LoRA LLM 응답:\")\n",
    "pprint.pprint(lora_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28206e1c",
   "metadata": {},
   "source": [
    "### 📏 정량적 성능 평가\n",
    "\n",
    "두 모델의 정확도를 간단하게 비교하기 위해 **텍스트 유사성**을 측정해봅시다.\n",
    "\n",
    "**평가 방법:**\n",
    "- 정답 JSON과 각 모델의 출력 JSON을 문자열로 변환\n",
    "- `difflib.SequenceMatcher`를 사용하여 유사성 점수 계산\n",
    "- 0.0 (완전히 다름) ~ 1.0 (완전히 같음) 범위로 평가\n",
    "\n",
    "이를 통해 fine-tuning이 실제로 성능 향상에 기여했는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa69f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 유사성 비교\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# JSON을 문자열로 변환하여 유사성 측정\n",
    "target_str = json.dumps(json.loads(json_content), sort_keys=True, ensure_ascii=False)\n",
    "lora_str = json.dumps(lora_output, sort_keys=True, ensure_ascii=False)\n",
    "naive_str = json.dumps(naive_output, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "# 유사성 계산\n",
    "lora_similarity = SequenceMatcher(None, target_str, lora_str).ratio()\n",
    "naive_similarity = SequenceMatcher(None, target_str, naive_str).ratio()\n",
    "\n",
    "print(\"=== 유사성 비교 결과 ===\")\n",
    "print(f\"LoRA 모델 유사성:  {lora_similarity:.4f}\")\n",
    "print(f\"Naive 모델 유사성: {naive_similarity:.4f}\")\n",
    "print(f\"차이: {lora_similarity - naive_similarity:.4f}\")\n",
    "\n",
    "if lora_similarity > naive_similarity:\n",
    "    improvement = ((lora_similarity - naive_similarity) / naive_similarity) * 100\n",
    "    print(f\"✅ LoRA 모델이 {improvement:.2f}% 더 우수합니다!\")\n",
    "else:\n",
    "    decline = ((naive_similarity - lora_similarity) / naive_similarity) * 100\n",
    "    print(f\"❌ Naive 모델이 {decline:.2f}% 더 우수합니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-factory-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
